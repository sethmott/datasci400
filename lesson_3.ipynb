{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to doing any sort of machine learning, we need to understand our data. This can mean making sore for example that:\n",
    "\n",
    "- know how to combine multiple data sources if we need to\n",
    "- the columns are what we think they are\n",
    "- the column types are correct\n",
    "- that we are aware of missing values or extreme values\n",
    "- that we are aware of the statistical **distribution** of the columns (**univariate** analysis)\n",
    "- that we are aware of how the columns in the data are related to each other (**bivariate or multivariate analysis**)\n",
    "\n",
    "We commonly refer to the above tasks as **EDA (exploratory data analysis)** and it is a very important task in data science, because it can guide us in how we need to process the data in preparation for modeling and what kinds of algorithms or modeling strategies the data lends itself to.\n",
    "\n",
    "Let's now read some data and start exploring it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cars = pd.read_csv(\"data/cars.csv\")\n",
    "cars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice what happens if we say `headers = None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = pd.read_csv(\"data/cars.csv\", header = None)\n",
    "cars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One very common task with data is to **subset** the data, also called **slicing** or **filtering**. We can subset data by rows, by columns, or both. We can subset data using numeric indices (positions), or using some kind of conditional logic. If we subset data using numeric indices, we need to remember that **in Python indexing starts at 0**, not at 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = pd.read_csv(\"data/cars.csv\")\n",
    "cars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by pulling some basic information about our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.shape # number of rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.dtypes # prints column types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.columns # column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.index # row indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some basic summary statistics about the columns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we can rename the columns in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col_names = [\"buying\", \"maintenance\", \"num_doors\", \"num_persons\", \"lug_boot\", \"safety\", \"evaluation\"]\n",
    "cars.columns = new_col_names\n",
    "cars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "There's another way we can rename columns by using the `DataFrame`'s `rename` method. Look at the documentation for it and write a line of code that renames the `evaluation` and `safety` columns to `eval` and `safe` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should not be surprised to find out that there is usually more than one way of doing things when it comes to many of the things we learn it this course. As much as possible, when you try to do something, it's good to first find out if the library (`pandas` in this case) has method for it, and if not then try to do things our own way.\n",
    "\n",
    "### End of exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `iloc` method to subset data by its index or position (named index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cars.iloc[4, 2] # show the 5th row, 3rd colum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.iloc[1:5, 1:3] # 2nd through 5th row, 2nd and 3rd column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsetting columns of the data using `iloc` is **not recommended**, because it makes the code hard to understand and because the position of a column in the data can change, which would break our code. It's much more common to subset columns using their names instead of their positions. For that we can use the `loc` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.loc[1:5, [\"maintenance\", \"num_doors\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how `loc` and `iloc` are methods, but their arguments are specified inside square brackets instead of parenthesis. This is done for historical reasons. Usually, methods are like functions and their arguments are inside parenthesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's another way we can subset the data by using the `filter` method instead of `loc` or `iloc`. In the case of filter, we need to specify the `axis` argument to say if the filter applies to rows (`axis = 0`) or columns (`axis = 1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.filter(items = range(1, 5), axis = 0).filter(items = [\"maintenance\", \"num_doors\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note from the below example that `filter` seems much more verbose and un-intuitive than using `loc` or `iloc`, but it has its advantages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Check the documentation for `filter` and use it to show the columns in the data that start with `num_`. Now do the same thing using `loc` and `cars.columns.str.startswith(\"num_\")]`. Which is easier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very common thing to do when working with tabular data is to point to a single column of it. For example, we may want to\n",
    "\n",
    "- change the type of a single column\n",
    "- do a log transformation on a single column\n",
    "- create a new column that is the sum of two other columns\n",
    "\n",
    "and so on. So based on what we learned so far, we know that we can use `loc` to point to a single column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.loc[:, \"num_persons\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But because pointing to single columns is such a common thing to do, `pandas` offers a shortcut for doing this. Instead of typing `data.loc[:, \"x\"]` to reference column `x` in `data`, we can use `data[\"x\"]` or `data.x`. Note that the second option is only possible if the column name does NOT contain any space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars[\"num_persons\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars[[\"num_persons\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.num_persons.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Notice that the type of `num_persons` is `object`. Why did that happen? It looks like it should be of type `integer` and not `object` (which is more appropriate for categorical data). Since the unique values for this columns are probably limited, list what the unique values are. You can use the `unique` method to get the unique values, or even better is to use the `value_counts` method, because it returns unique values and counts for each. Try both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we're at it, let's write a loop to get unique counts for each of the columns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw how we can use `value_counts` to get unique values and their counts for some column, also called a **one-way table**. What if we want counts for all the combinations of two or several columns? In statistics, we call this a **two-way table**. We can use the `pd.crosstab` function to get this. Setting `margins = True` we can add row and column sums to our two-way table. In statistics these are called **marginal sums**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(cars[\"num_doors\"], cars[\"num_persons\"], margins = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now read another data set. Don't worry, this one is also about cars!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "auto = pd.read_csv('data/auto-mpg.csv', sep = '\\s+', header = None, \n",
    "                   names = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', \n",
    "                            'acceleration', 'model year', 'origin', 'car_name'])\n",
    "auto.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Find out how many rows and columns the data has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out how you can sort a `DataFrame` object and sort it by `cylinders` and descending `weight`. See if you can find out what the `inplace` argument does. This is a commom argument that many `DataFrame` methods have. It's important to be aware of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's talk about data visualization, or **data viz** as the kids call it. There are two common libraries for data viz. There are two main libraries for producing visualizations: `matplotlib` and `seaborn`. The `matplotlib` library is more **low-level** meaning you usually have to write more code to produce and customize your plot. The `seaborn` library is more **high-level** meaning that the plots usually look pretty good without too much work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you need to run this to produce visualizations in a jupyter notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(auto['mpg']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.distplot(auto[\"mpg\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `seaborn` library has a function called `pairplot` which can be used to get a **scatter plot matrix**, which is a matrix whose diagonal elements are histograms of each column in the data and whose off-diagonal elements are scatter plots of any pair of columns in the data. A histogram is what we call a **univariate** visualization, i.e. summarizes a single variable, and a scatter plot is a **bivariate** visualization, because it shows the relationship between two variables. \n",
    "\n",
    "Note that for **large datasets**, plotting scatter plots is usually **not a good idea** because it can be very slow. So if the data is very large use the `sample` method to take a sample of it before you plot any scatter plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(auto.sample(100));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the scatter plot matrix, we can also show the **correlation matrix** by just calling `corr` on the `DataFrame`, which is the correlation of any pairs of numeric columns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw earlier how we can use `loc` to subset the data. We're going to use it here again, but this time we're subsetting the data by rows that meet a certain **condition**. In this case, the condition is `auto['horsepower'] == '?'`, which means cases where `horsepower` is the string `?`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hp_na = auto['horsepower'] == '?'\n",
    "wt_gt_2500 = auto['weight'] > 2500\n",
    "auto.loc[hp_na & wt_gt_2500, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make add additional conditions, but need to wrap each condition in parentheses and separate them by and (`&`) and or (`|`). Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto.loc[(auto['horsepower'] == '?') & (auto['acceleration'] > 17)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At some point, with lots of conditions, the code starts to look nasty, so it's good to refactor it to make it more readable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_missing = auto['horsepower'] == '?'\n",
    "acc_gt_17 = auto['acceleration'] > 17\n",
    "auto.loc[hp_missing & acc_gt_17] # the parenthesis are not needed anymore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `~` to negate a condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto.loc[hp_missing & ~acc_gt_17]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Plot the distribution of `horsepower`. Are you getting an error? There's a good reason: as we saw earlier, some of the rows have the string `?` for `horsepower`. So we need to convert `horsepower` into a numeric column. We can do that using the function `pd.to_numeric` (pay attention to the `errors` argument).\n",
    "\n",
    "Convert `horsepower` to type `numeric` and then plot its distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that `horsepower` has missing values, although the data uses the string `?` to mark them as missing. This is not a good idea, because as the previous exercise showed we run into problems when we want to analyze or visualize the column. For example let's see what the average value is for `horsepower`. There are two identical ways we can get the average of a column:\n",
    "\n",
    "- we can call the `mean` method on the column\n",
    "- we can use the `mean` function in numpy\n",
    "\n",
    "Once again, which is best is a matter of preference. Uncomment the code below and run it. You'll notice that we get an error. Note that you can use `CTRL+/` to comment and un-comment code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto['horsepower'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.mean(auto['horsepower'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we need to to convert `horsepower` to numeric so that we can get the average. We saw one way of doing that using `pd.to_numeric`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto['horsepower'] = pd.to_numeric(auto['horsepower'], errors = \"coerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what exactly happened to the rows where `horsepower` is `?`? They were converted into `NaN` which stands for \"Not a Number\". It is similar to null values in SQL tables. We can use the `isnull` method to find rows with missing values, which works with both numeric and categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "auto.loc[auto['horsepower'].isnull(), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get the average and the median for `horsepower`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto['horsepower'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto['horsepower'].median() # the median is the \"middle value\" if we sort the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many machine learning algorithms do not like missing values. As such, we need to deal with the missing values prior to passing the data to the ML algorithm. There are two main ways we can do so: drop them from the data, or impute the missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could throw out any rows with missing data: This is a very **conservative** approach and should only be taken if we have lots of data and very few rows with missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_nona = auto.dropna() # this is how we drop any rows with nulls\n",
    "auto_nona.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Instead of dropping rows where **any** column is missing, we can drop rows where **certain** columns are missing. Examine `dropna` and see how you can modify it so you drop only the rows for where `horsepower` **and** `displacement` are missing (notice that we said **and**, not **or**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of dropping the missing values, we can **impute** them: By imputing, we mean replacing the missing values with something that makes sense. Turns out there's a lot of ways to define what \"makes sense\", but one simple approach is to replace missing values for each column with the mean or median for that column. Of couse, this only works with numeric columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Replace the missing values for `horsepower` with the median value.\n",
    "\n",
    "HINT: \n",
    "- You use `isnull` to find out if `horsepower` is missing.\n",
    "- You use `loc` to find the subset of the data where `horsepower` is missing.\n",
    "- You use `median` to find the median value for `horsepower`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now you should be able to use `pandas` to do a lot of data processing, mostly using one or two lines of code. Make sure you practice the content of this notebook a few times so you're comfortable with all of it. We will return to these concepts over an over again in future lectures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
